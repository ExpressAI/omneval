import torch
from omneval.tasks import BaseEvaluator
from omneval.utils import get_logits, adjust_length, collate_fn, merge_fn
from omneval.registry import register_evaluator
from transformers import AutoTokenizer, AutoModelForPreTraining, AutoModelForCausalLM, AutoModelForSeq2SeqLM
from torch.nn.functional import cross_entropy
import pdb
import collections
from torch.utils.data import DataLoader
import logging
from omneval.utils import BERT_MODELS, GPT_MODELS, BART_MODELS, T5_MODELS
from tqdm import tqdm

class BaseEvaluatorForGenerateQA(BaseEvaluator):

    def build_model(self):
        return AutoModelForPreTraining.from_pretrained(self.config.arch).to(self.device)

    def preprocessing(self, dataset, **kwargs):
        kwargs['topk'] = getattr(self.config, 'topk', 3)
        return dataset, kwargs

    def parse_predictions(self, prediction):
        return prediction

    def analysis(self, res_list):
        return {}

    def eval(self, dataset, metrics_fn, **kwargs):
        """
        (The only change is to retain id and change the format of labels and prediction)
        Generate the evaluation metrics and analysis (call: self.decode function)
        :param dataset: dataset generated by the processor's `generate_dataset`
        :param kwargs: other input generated by the processor's `generate_aux_input`
        :return: The evaluation metrics
        """
        self.model.eval()
        dataset, kwargs = self.preprocessing(dataset, **kwargs)
        dataloader = DataLoader(dataset, batch_size=self.config.eval_batch_size, collate_fn=lambda x: collate_fn(x, exclude=[self.label_name, 'id']))
        labels = []
        ids = []
        predictions = []
        res = collections.defaultdict(list)
        for batch in tqdm(dataloader):
            label = batch.pop(self.label_name)
            id_ = batch.pop('id')
            batch = {k: v.to(self.device) for k, v in batch.items()}
            prediction = self.decode(batch, **kwargs)
            labels += label
            ids += id_
            predictions += prediction['predictions']
        # TODO: need to modify this part
        predictions_squad_format = [{'id': id_, 'prediction_text': pred} for id_, pred in zip(ids, predictions)]
        label_squad_format = [
            {'id': id_, 'answers': {'text': answer.get('text', []), 'answer_start': answer.get('answer_start', [])}}
            for id_, answer in zip(ids, labels)]
        metrics = metrics_fn.compute(predictions=predictions_squad_format, references=label_squad_format, **self.config.metrics_kwargs)
        res['label'] = labels
        res['id'] = ids
        res['predictions'] = predictions
        res_list = []
        length = len(res['predictions'])
        for i in range(length):
            res_list.append(self.parse_predictions({k: v[i] for k, v in res.items()}))
        eval_result = {'plm': self.config.arch}
        eval_result.update(metrics)
        eval_result.update(self.analysis(res_list))
        return res_list, eval_result


@register_evaluator('question_answering', BART_MODELS+T5_MODELS)
class BARTEvaluatorForGenerateQA(BaseEvaluatorForGenerateQA):

    def decode(self, batch, **kwargs):
        with torch.no_grad():
            outputs = self.model.generate(**batch, num_beams=self.config.num_beams, early_stopping=True, max_length=self.config.decode_max_length)
        pred = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        predictions = {
            'predictions': pred,
        }
        return predictions


@register_evaluator('question_answering', GPT_MODELS)
class GPTEvaluatorForGenerateQA(BaseEvaluatorForGenerateQA):

    def decode(self, batch, **kwargs):
        with torch.no_grad():

            outputs = self.model.generate(**batch, num_beams=self.config.num_beams,
                                          early_stopping=True,
                                          max_length=adjust_length(self.config)+self.config.decode_max_length)
        preds = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        source = self.tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)
        new_pred = []
        for i in range(len(preds)):
            new_pred.append(preds[i][len(source[i]): ])
        predictions = {
            'predictions': new_pred,
        }
        return predictions